---
layout: default
title: Datasets 
parent: Behavioral Testing 
nav_order: 5
---

# Datasets

This page is dedicated to providing links to different stimulus databases that might be relevant for members of the lab.

## Scenes

* [Natural Scenes Dataset (NSD)](https://naturalscenesdataset.org/) - The Natural Scenes Dataset (NSD) is a large-scale fMRI dataset conducted at ultra-high-field (7T) strength.
* [Places Database](http://places.csail.mit.edu/) - Scene recognition database with 205 scene categories and 2.5 millions of images with a category label.
* [SUN Database: Scene Categorization Benchmark](https://vision.princeton.edu/projects/2010/SUN/) - 899 categories and 130,519 images
    * [FIne-GRained Image Memorability](http://figrim.mit.edu/) - Images from the SUN database but including memorability scores
        * [FIGRIM Fixation Dataset](http://figrim.mit.edu/index_eyetracking.html) - Images from the dataset above (FIGRIM) including eye gaze data
* [Large-scale Image Memorability](http://memorability.csail.mit.edu/) - annotated image memorability dataset (containing 60,000 images from diverse sources)
* [Google's Open Images Dataset](https://storage.googleapis.com/openimages/web/index.html) - Object oriented scenes (with descriptions)


## Objects 

* [ImageNet](https://www.image-net.org/) - Does it even need a description?
    * [EcoSet](https://www.kietzmannlab.org/ecoset/) - 1.5m images from 565 basic level categories, chosen to be both (i) frequent in linguistic usage, and (ii) rated by human observers as concrete. A big subset of images comes from ImageNet.
* [THINGS Dataset](https://things-initiative.org/) - A freely available database of 26,107 high quality, manually-curated images of 1,854 diverse object concepts
* [COCO Dataset](https://cocodataset.org/#home) - Common Objects in Context: large-scale object detection, segmentation, and captioning dataset.
* [Google's Line Drawings Dataset](https://quickdraw.withgoogle.com/data)
* [MNIST dataset](http://yann.lecun.com/exdb/mnist/) - Do we need any explanation?


## Faces

* [10k Adult US Faces](https://www.wilmabainbridge.com/facememorability2.html)
* [List of Face Datasets following different purposes](https://rystoli.github.io/FSTC.html#stim) - A list in a list. There is even a dataset from the Radboud University in there!

## Language 

* [Mother of Unification Studies](https://www.nature.com/articles/s41597-019-0020-y) - A 204-subject multimodal neuroimaging dataset to study language processing. All subjects performed a language task, during which they processed linguistic utterances that either consisted of normal or scrambled sentences. Half of the subjects were reading the stimuli, the other half listened to the stimuli.
* [Narratives Dataset](https://www.nature.com/articles/s41597-021-01033-3) - Dataset containing the fMRI recordings of 345 individuals listening to 27 spoken stories in English, from 7 to 56 min (4.6 h of unique stimulus in total).

## Dynamic Stimuli

* Ingmar's Dataset of Ballet Dancers (ask [him](https://x.com/ingmarvries?lang=es) for it) - The stimulus set consisted of videos of 14 unique ballet dancing sequences, each consisting of four smoothly connected ballet figures selected from a pool of five unique figures
